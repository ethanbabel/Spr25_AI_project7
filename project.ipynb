{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96aa469d",
   "metadata": {},
   "source": [
    "# Project 6: Solving Markov Decision Processes with Value Iteration\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this project, you will learn about **Markov Decision Processes (MDPs)**, one of the fundamental models in artificial intelligence for decision-making under uncertainty.  \n",
    "You will implement **Value Iteration**, a classic dynamic programming algorithm for solving MDPs.\n",
    "\n",
    "This project will guide you step-by-step through:\n",
    "- Setting up an MDP environment\n",
    "- Implementing Value Iteration\n",
    "- Extracting the optimal policy\n",
    "- Running and visualizing your results\n",
    "\n",
    "You are expected to fill in each **TODO** where indicated.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0189804f",
   "metadata": {},
   "source": [
    "## 1. MDP Basics and Setup\n",
    "\n",
    "An MDP is defined by:\n",
    "- A set of states **S**\n",
    "- A set of actions **A**\n",
    "- A transition probability function **P(s' | s, a)**\n",
    "- A reward function **R(s, a, s')**\n",
    "- A discount factor **gamma (Œ≥)**\n",
    "\n",
    "The goal is to find an **optimal policy** œÄ\\* that maximizes expected cumulative rewards.\n",
    "\n",
    "## 1.1 Our Gridworld Environment\n",
    "\n",
    "To make the concepts of MDPs concrete, we will work with a **6√ó6 Gridworld** environment.\n",
    "\n",
    "The environment includes:\n",
    "- **üèÅ Terminal State**: (5,5) ‚Äî reaching this state yields a large positive reward (+10).\n",
    "- **üß± Walls**: Some cells are blocked and cannot be entered.\n",
    "- **üíÄ Traps**: Certain states (like (3,2) and (4,4)) carry severe penalties (-5 or -10) if the agent moves into them.\n",
    "- **üíé Mini Bonuses**: Other states (like (1,5)) offer small positive rewards (+3), tempting the agent to detour.\n",
    "- **‚¨ú Empty Cells**: Normal grid cells, with a small step penalty (-1) for moving through them.\n",
    "\n",
    "Additionally:\n",
    "- **Actions are stochastic**: Intended moves succeed 60% of the time, while 20% of the time the agent \"slips\" perpendicularly to the intended direction by mistake.\n",
    "- **Obstacles must be navigated carefully**, because risky moves could accidentally slip the agent into traps.\n",
    "\n",
    "### Why This Environment?\n",
    "\n",
    "This world was carefully designed to teach key ideas of MDP planning:\n",
    "- **Tradeoffs**: The agent must decide between faster but riskier paths and longer safer routes. Every step is penalized with a reward of -1, so agents are encouraged to find the fastest path to the goal. However, the gridworld is intentionally designed such that the fastest path is littered with obstacles and traps, making that path dangerous and incentivizing agents to take longer routes. \n",
    "- **Uncertainty**: Actions are not guaranteed, so planning must account for the possibility of slipping.\n",
    "- **Reward Optimization**: Sometimes detouring for small bonuses can be optimal, depending on their location and reward magnitude.\n",
    "- **Strategic Decision-Making**: Simply following the shortest path is no longer always the best strategy.\n",
    "\n",
    "By solving this Gridworld using **Value Iteration**, you will experience firsthand how agents make optimal decisions in complex, uncertain environments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0b7bb06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚¨ú ‚¨ú ‚¨ú ‚¨ú ‚¨ú ‚¨ú ‚¨ú\n",
      "‚¨ú üß± üíÄ ‚¨ú ‚¨ú üíé ‚¨ú\n",
      "‚¨ú ‚¨ú ‚¨ú ‚¨ú ‚¨ú ‚¨ú ‚¨ú\n",
      "‚¨ú ‚¨ú üíÄ ‚¨ú ‚¨ú ‚¨ú ‚¨ú\n",
      "‚¨ú ‚¨ú ‚¨ú üß± üíÄ ‚¨ú ‚¨ú\n",
      "‚¨ú ‚¨ú ‚¨ú ‚¨ú üíÄ ‚¨ú üèÅ\n"
     ]
    }
   ],
   "source": [
    "# Setup: Define a simple Gridworld MDP\n",
    "import numpy as np\n",
    "\n",
    "class GridworldMDP:\n",
    "    def __init__(self, grid_size=(6,7), terminal_states=[(5,6)], walls=[(1,1), (4,3)], traps={(1, 2): -10 ,(3, 2): -10,(4, 4): -10,(5, 4): -10}, mini_bonuses={(1, 5): 3}, gamma=0.9):\n",
    "        self.grid_size = grid_size\n",
    "        self.terminal_states = terminal_states\n",
    "        self.walls = walls\n",
    "        self.traps = traps\n",
    "        self.mini_bonuses = mini_bonuses\n",
    "        self.gamma = gamma\n",
    "        self.actions = ['up', 'down', 'left', 'right']\n",
    "        self.states = [(i, j) for i in range(grid_size[0]) for j in range(grid_size[1])]\n",
    "        \n",
    "    def get_actions(self, state):\n",
    "        if state in self.terminal_states or state in self.walls:\n",
    "            return []\n",
    "        return self.actions\n",
    "\n",
    "    def get_next_state(self, state, action):\n",
    "        if state in self.terminal_states:\n",
    "            return state\n",
    "        \n",
    "        i, j = state\n",
    "        if action == 'up':\n",
    "            i = max(i-1, 0)\n",
    "        elif action == 'down':\n",
    "            i = min(i+1, self.grid_size[0]-1)\n",
    "        elif action == 'left':\n",
    "            j = max(j-1, 0)\n",
    "        elif action == 'right':\n",
    "            j = min(j+1, self.grid_size[1]-1)\n",
    "        \n",
    "        next_state = (i, j)\n",
    "        \n",
    "        if next_state in self.walls:\n",
    "            return state  # Can't move into a wall, stay in the same place\n",
    "        return next_state\n",
    "\n",
    "    def get_transition_probs(self, state, action):\n",
    "        \"\"\"Returns a list of (next_state, probability) pairs.\"\"\"\n",
    "        if state in self.terminal_states or state in self.walls:\n",
    "            return [(state, 1.0)]\n",
    "\n",
    "        intended = action\n",
    "        perpendicular = {\n",
    "            'up': ['left', 'right'],\n",
    "            'down': ['left', 'right'],\n",
    "            'left': ['up', 'down'],\n",
    "            'right': ['up', 'down'],\n",
    "        }\n",
    "        \n",
    "        probs = [\n",
    "            (self.get_next_state(state, intended), 0.6),\n",
    "            (self.get_next_state(state, perpendicular[action][0]), 0.2),\n",
    "            (self.get_next_state(state, perpendicular[action][1]), 0.2),\n",
    "        ]\n",
    "        \n",
    "        return probs\n",
    "\n",
    "    def get_reward(self, state, action, next_state):\n",
    "        if next_state in self.terminal_states:\n",
    "            return 10 # Reward for reaching the terminal state\n",
    "        elif next_state in self.traps.keys():\n",
    "            return self.traps[next_state] # Penalty for falling into a trap\n",
    "        elif next_state in self.mini_bonuses.keys():\n",
    "            return self.mini_bonuses[next_state] # Bonus for reaching a mini bonus\n",
    "        else:\n",
    "            return -1 # Default step penalty\n",
    "        \n",
    "    def print_gridworld(mdp):\n",
    "        \"\"\"\n",
    "        Prints the Gridworld layout showing walls, traps, mini bonuses, and goal states.\n",
    "        \"\"\"\n",
    "        grid = []\n",
    "        for i in range(mdp.grid_size[0]):\n",
    "            row = []\n",
    "            for j in range(mdp.grid_size[1]):\n",
    "                pos = (i, j)\n",
    "                if pos in mdp.walls:\n",
    "                    row.append('üß±')  # Wall\n",
    "                elif pos in mdp.terminal_states:\n",
    "                    row.append('üèÅ')  # Goal (terminal state)\n",
    "                elif pos in mdp.traps.keys():\n",
    "                    row.append('üíÄ')  # Trap (negative reward)\n",
    "                elif pos in mdp.mini_bonuses.keys():\n",
    "                    row.append('üíé')  # Mini bonus (small positive reward)\n",
    "                else:\n",
    "                    row.append('‚¨ú')  # Empty space\n",
    "            grid.append(row)\n",
    "        \n",
    "        # Pretty print the grid\n",
    "        for row in grid:\n",
    "            print(' '.join(row))\n",
    "        \n",
    "\n",
    "mdp = GridworldMDP()\n",
    "mdp.print_gridworld()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb54f3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: Print the Policy Nicely\n",
    "def print_policy(policy, grid_size, walls):\n",
    "    arrow_map = {\n",
    "        'up': '‚¨ÜÔ∏è', 'down': '‚¨áÔ∏è', 'left': '‚¨ÖÔ∏è', 'right': '‚û°Ô∏è', \n",
    "        None: 'üö´'\n",
    "    }\n",
    "    \n",
    "    for i in range(grid_size[0]):\n",
    "        row = ''\n",
    "        for j in range(grid_size[1]):\n",
    "            if (i,j) in walls:\n",
    "                row += 'üß± '\n",
    "            else:\n",
    "                row += arrow_map[policy.get((i,j), None)] + ' '\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6a22fc",
   "metadata": {},
   "source": [
    "# 2. Value Iteration Overview\n",
    "\n",
    "Value Iteration works by repeatedly updating the value of each state using the Bellman optimality equation:\n",
    "\n",
    "```math\n",
    "V(s) = \\max_a \\sum_{s'} P(s'|s,a) [R(s,a,s') + \\gamma V(s')]\n",
    "```\n",
    "\n",
    "In our simple Gridworld:\n",
    "- Assume transitions are **deterministic**: each action deterministically moves the agent.\n",
    "- Reward is **-1** for each move, and **0** if at a terminal state.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3c3482f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1. TODO: Initialize the Value Table\n",
    "# Create a dictionary mapping each state to a value (start with zero)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "V = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7283a094",
   "metadata": {},
   "source": [
    "# 3. Implementing Value Iteration\n",
    "\n",
    "At each iteration:\n",
    "- For every state:\n",
    "  - Look at all possible actions\n",
    "  - Calculate expected value of each action\n",
    "  - Take the maximum value\n",
    "- Stop when the change is below a small threshold (convergence)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2003baf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1. TODO: Implement Value Iteration\n",
    "\n",
    "def value_iteration(mdp, V, epsilon=1e-4):\n",
    "    # TODO: Implement the value iteration algorithm\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc426d1",
   "metadata": {},
   "source": [
    "# 4. Extract the Optimal Policy\n",
    "\n",
    "Once you have the Value function V, you can extract the optimal policy by:\n",
    "- For each state:\n",
    "  - Choose the action that leads to the best expected value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60f77b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1. TODO: Implement Policy Extraction\n",
    "\n",
    "def extract_policy(mdp, V):\n",
    "    # TODO: Implement the policy extraction from the value function\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6e2bf4",
   "metadata": {},
   "source": [
    "## 5. Print the Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1229506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Print the gridworld again (for comparison) and policy nicely (use the provided print_policy function)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2f672c",
   "metadata": {},
   "source": [
    "## 6. Explain Your Learned Policy (Short Answer)\n",
    "\n",
    "After running value iteration and extracting the optimal policy, you should observe interesting patterns in the agent‚Äôs behavior.\n",
    "\n",
    "**Question:**  \n",
    "- In your own words, explain the key decisions the agent makes when navigating the Gridworld.\n",
    "- Specifically, why does the agent sometimes avoid certain paths even if they seem direct?\n",
    "- Why might the agent sometimes go out of its way to collect the mini bonus, and other times ignore it?\n",
    "\n",
    "Write 4‚Äì6 sentences interpreting the learned policy based on the layout of traps, mini bonuses, and transition uncertainty."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf62c19",
   "metadata": {},
   "source": [
    "### TODO: Analyze your learned policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966cba92",
   "metadata": {},
   "source": [
    "##  Bonus 1: Plotting Value Iteration Convergence (Optional)\n",
    "\n",
    "During Value Iteration, the value function gradually stabilizes as updates become smaller and smaller.\n",
    "\n",
    "**Task:**  \n",
    "- Modify your `value_iteration` function to **record** the maximum change (delta) at each iteration.\n",
    "- After Value Iteration finishes, **plot delta vs iteration number** using `matplotlib`.\n",
    "\n",
    "**Goal:**  \n",
    "You should observe that delta decreases over time, ideally exponentially, as the value function converges.\n",
    "\n",
    "**Hint:**  \n",
    "- Plot the convergance on a log scale for better visability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f12ed5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BONUS 1 TODO: Plot the convergence of the value function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a0804e",
   "metadata": {},
   "source": [
    "## Bonus 2: How Does Discount Factor (Œ≥) Affect Convergence? (Optional)\n",
    "\n",
    "The discount factor **Œ≥** controls how much the agent values future rewards compared to immediate rewards.\n",
    "\n",
    "**Task:**  \n",
    "- Try re-running your Value Iteration with different values of Œ≥:\n",
    "  - Œ≥ = 0.5\n",
    "  - Œ≥ = 0.9 (default)\n",
    "  - Œ≥ = 0.99\n",
    "- For each Œ≥, plot the delta vs iteration curve as you did before.\n",
    "\n",
    "**Questions to Think About:**\n",
    "- How does the value of Œ≥ affect the number of iterations needed for convergence?\n",
    "- Why do higher Œ≥ values tend to require more iterations?\n",
    "- What does this tell you about agents that prioritize long-term rewards versus short-term gains?\n",
    "\n",
    "**Goal:**  \n",
    "Understand how **temporal horizon** (how far into the future the agent plans) affects the speed and difficulty of finding optimal policies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4fa6ba99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bonus 2 TODO: Compare convergence for different discount factors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32487613",
   "metadata": {},
   "source": [
    "### Bonus 2 TODO: Provide a Brief Explanation of Your Graph"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
